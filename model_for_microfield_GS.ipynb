{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e458fde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Example of multiclass prediction using text variables\n",
    "# https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('C:/Users/melyg/Desktop/Networks/Data/fields_for_micro.csv')\n",
    "print(df.shape)\n",
    "#df = df1.sample(frac =.6)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24658040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need only two columns, cleaning and dictionaries\n",
    "from io import StringIO\n",
    "\n",
    "col = ['field_micro', 'pubjournal', 'pubtitle']\n",
    "df = df[col]\n",
    "df = df[pd.notnull(df['pubtitle'])] #removes null values\n",
    "df = df[pd.notnull(df['pubjournal'])] #removes null values\n",
    "df.columns = ['field_micro', 'pubjournal', 'pubtitle']\n",
    "df['category_id'] = df['field_micro'].factorize()[0] #Assigns a number to categories starting at zero\n",
    "category_id_df = df[['field_micro', 'category_id']].drop_duplicates().sort_values('category_id') #Descriptive table of number of categories and number of cases\n",
    "category_to_id = dict(category_id_df.values) #dictionary of categories\n",
    "id_to_category = dict(category_id_df[['category_id', 'field_micro']].values)  #dictionary of categories\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fcc19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking \n",
    "array = df['category_id'].to_numpy()\n",
    "print(np.unique(array, return_counts=True))\n",
    "frequency = df.category_id.value_counts()\n",
    "print(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb76cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imbalanced clases (visualization)\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "df.groupby('field_micro').pubtitle.count().plot.bar(ylim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1a62e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text representation: texts are converted to a more manageable representation\n",
    "#Use the bag of words model: a model where for each document (text var) the presence (and often the frequency) of words is taken into consideration, but the order in which they occur is ignored\n",
    "#we will calculate a measure called Term Frequency, Inverse Document Frequency, abbreviated to tf-idf using \"sklearn.feature_extraction.text.TfidfVectorizer\"\n",
    "\n",
    "# sublinear_df is set to True to use a logarithmic form for frequency.\n",
    "# min_df is the minimum numbers of documents a word must be present in to be kept.\n",
    "# norm is set to l2, to ensure all our feature vectors have a euclidian norm of 1.\n",
    "# ngram_range is set to (1, 2) to indicate that we want to consider both unigrams and bigrams.\n",
    "# stop_words is set to \"english\" to remove all common pronouns (\"a\", \"the\", ...) to reduce the number of noisy features.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "features1 = tfidf.fit_transform(df.pubtitle).toarray()\n",
    "labels = array\n",
    "print(features1.shape)\n",
    "del array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bbf976",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use sklearn.feature_selection.chi2 to find the terms that are the most correlated with each of the products\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "N = 2\n",
    "for field_micro, category_id in sorted(category_to_id.items()):\n",
    "  features_chi2_1 = chi2(features1, labels == category_id)\n",
    "  indices_1 = np.argsort(features_chi2_1[0])\n",
    "  feature_names_1 = np.array(tfidf.get_feature_names_out())[indices_1]\n",
    "  unigrams_1 = [v for v in feature_names_1 if len(v.split(' ')) == 1]\n",
    "  bigrams_1 = [v for v in feature_names_1 if len(v.split(' ')) == 2]\n",
    "  print(\"# '{}':\".format(field_micro))\n",
    "  print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams_1[-N:])))\n",
    "  print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams_1[-N:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbffcf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do same for pubjournal\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "features2 = tfidf.fit_transform(df.pubjournal).toarray()\n",
    "print(features2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53439223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Do same for pubjournal\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "N = 2\n",
    "for field_micro, category_id in sorted(category_to_id.items()):\n",
    "  features_chi2_2 = chi2(features2, labels == category_id)\n",
    "  indices_2 = np.argsort(features_chi2_2[0])\n",
    "  feature_names_2 = np.array(tfidf.get_feature_names_out())[indices_2]\n",
    "  unigrams_2 = [v for v in feature_names_2 if len(v.split(' ')) == 1]\n",
    "  bigrams_2 = [v for v in feature_names_2 if len(v.split(' ')) == 2]\n",
    "  print(\"# '{}':\".format(field_micro))\n",
    "  print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams_2[-N:])))\n",
    "  print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams_2[-N:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2298c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating one array for features\n",
    "features_final = np.concatenate((features1, features2), axis=1)\n",
    "features_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd851aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiclass\n",
    "#Naive Bayes Classifier (using only pubtitle)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['pubtitle'], df['field_micro'], random_state = 0)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3d425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clf.predict(count_vect.transform([\"a regulatory policy strategy for protecting immigrant workers\"])))\n",
    "print(clf.predict(count_vect.transform([\"a relative question: the developing world is reevaluating what it means to be poor\"])))\n",
    "print(clf.predict(count_vect.transform([\"10 central bank independence: growing threats\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a419bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting previous files to deal with memory problems\n",
    "del df1\n",
    "del features1, features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a501179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "   #LogisticRegression(random_state=0),\n",
    "]\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model, features_final, labels, scoring='accuracy', cv=CV)\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "\n",
    "import seaborn as sns\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d61b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df.groupby('model_name').accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4470dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the best model\n",
    "model = LinearSVC()\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features_final, labels, df.index, test_size=0.25, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=category_id_df.field_micro.values, yticklabels=category_id_df.field_micro.values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4537f5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "for predicted in category_id_df.category_id:\n",
    "  for actual in category_id_df.category_id:\n",
    "    if predicted != actual and conf_mat[actual, predicted] >= 10:\n",
    "      print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual], id_to_category[predicted], conf_mat[actual, predicted]))\n",
    "      display(df.loc[indices_test[(y_test == actual) & (y_pred == predicted)]])\n",
    "      print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a69639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c1f5e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fieldsgs",
   "language": "python",
   "name": "fieldsgs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
